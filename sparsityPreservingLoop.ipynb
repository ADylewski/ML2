{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ae64dac-03cc-42f7-ae49-8b46eea22242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b35815a-08c7-4888-805a-a7c41b2a6ac8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images, labels = zip(*batch)  # Separate images and labels\n",
    "\n",
    "    # Stack images into a single tensor\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    # Prepare targets (handle empty label cases)\n",
    "    batch_idx = []\n",
    "    cls_labels = []\n",
    "    bboxes = []\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        if len(label) > 0:\n",
    "            batch_idx.append(torch.full((label.shape[0], 1), i, dtype=torch.float32))  # Batch index\n",
    "            cls_labels.append(label[:, 0].unsqueeze(1))  # Class ID\n",
    "            bboxes.append(label[:, 1:])  # Bounding boxes (cx, cy, w, h)\n",
    "\n",
    "    if len(batch_idx) > 0:\n",
    "        batch_idx = torch.cat(batch_idx).to(images.device)\n",
    "        cls_labels = torch.cat(cls_labels).to(images.device)\n",
    "        bboxes = torch.cat(bboxes).to(images.device)\n",
    "    else:\n",
    "        batch_idx = torch.empty((0, 1), dtype=torch.float32).to(images.device)\n",
    "        cls_labels = torch.empty((0, 1), dtype=torch.float32).to(images.device)\n",
    "        bboxes = torch.empty((0, 4), dtype=torch.float32).to(images.device)\n",
    "\n",
    "    batch_dict = {\n",
    "        \"img\": images,\n",
    "        \"batch_idx\": batch_idx,\n",
    "        \"cls\": cls_labels,\n",
    "        \"bboxes\": bboxes\n",
    "    }\n",
    "    \n",
    "    return batch_dict\n",
    "\n",
    "\n",
    "class YoloDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, img_size=640, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png'))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, self.image_files[index])\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
    "        img = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1) / 255.0  # Normalize to [0,1]\n",
    "\n",
    "        # Load label (NO CONVERSION)\n",
    "        label_path = os.path.join(self.label_dir, self.image_files[index].replace(\".jpg\", \".txt\").replace(\".png\", \".txt\"))\n",
    "        labels = torch.zeros((0, 5), dtype=torch.float32)  # Default empty tensor for images without labels\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            labels = torch.tensor([list(map(float, line.strip().split())) for line in open(label_path)], dtype=torch.float32)\n",
    "\n",
    "        return img, labels  # Labels remain in (class_id, x_center, y_center, width, height) format\n",
    "\n",
    "# Paths\n",
    "train_img_dir = \"../datasets/cocoa_diseases/images/train\"\n",
    "train_label_dir = \"../datasets/cocoa_diseases/labels/train\"\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = YoloDataset(train_img_dir, train_label_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "efcb2881-479a-4120-953a-445e8888e108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def match_targets_to_grid(targets, grid_size, num_classes):\n",
    "    \"\"\"\n",
    "    Converts target bounding boxes to the YOLO feature map format.\n",
    "\n",
    "    Args:\n",
    "        targets: (num_objects, 6) tensor with (batch_idx, class_id, x_center, y_center, width, height).\n",
    "        grid_size: The grid size (e.g., 80 for 80x80 feature maps).\n",
    "        num_classes: Total number of object classes.\n",
    "\n",
    "    Returns:\n",
    "        target_map: Tensor shaped (batch_size, num_classes + 5, grid_size, grid_size)\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = int(targets[:, 0].max().item()) + 1  # Get batch size from batch index\n",
    "    target_map = torch.zeros((batch_size, num_classes + 5, grid_size, grid_size), dtype=torch.float32)\n",
    "\n",
    "    for obj in targets:\n",
    "        batch_idx, class_id, x, y, w, h = obj.tolist()\n",
    "        batch_idx = int(batch_idx)\n",
    "        class_id = int(class_id)\n",
    "\n",
    "        # Convert to grid coordinates\n",
    "        grid_x = int(x * grid_size)\n",
    "        grid_y = int(y * grid_size)\n",
    "\n",
    "        # Normalize box coordinates to the cell level\n",
    "        x_cell = (x * grid_size) - grid_x\n",
    "        y_cell = (y * grid_size) - grid_y\n",
    "        w_cell = w * grid_size\n",
    "        h_cell = h * grid_size\n",
    "\n",
    "        # Set the target values in the grid cell\n",
    "        target_map[batch_idx, class_id, grid_y, grid_x] = 1  # One-hot class label\n",
    "        target_map[batch_idx, -5:, grid_y, grid_x] = torch.tensor([1, x_cell, y_cell, w_cell, h_cell])  # Objectness + bbox\n",
    "\n",
    "    return target_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cb9ea266-4618-4bae-9dc1-08304802a2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomYOLOLoss(nn.Module):\n",
    "    def __init__(self, num_classes, grid_size=80):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.grid_size = grid_size\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        \"\"\"\n",
    "        Compute loss between model predictions and ground truth.\n",
    "        Args:\n",
    "            preds: List of 3 tensors (YOLO outputs at different scales)\n",
    "            targets: (num_objects, 6) tensor with (batch_idx, class_id, x_center, y_center, width, height)\n",
    "        \"\"\"\n",
    "        # Extract first scale output for simplicity\n",
    "        preds = preds[0]  # Shape: (batch, 67, 80, 80)\n",
    "        batch_size = preds.shape[0]\n",
    "\n",
    "        # Convert targets to grid format\n",
    "        target_map = match_targets_to_grid(targets, self.grid_size, self.num_classes).to(preds.device)\n",
    "\n",
    "        # Split predictions into class scores, objectness, and bounding boxes\n",
    "        pred_cls = preds[:, :-5, :, :]  # Class predictions\n",
    "        pred_obj = preds[:, -5, :, :]   # Objectness score\n",
    "        pred_boxes = preds[:, -4:, :, :]  # Bounding box predictions\n",
    "\n",
    "        # Split targets the same way\n",
    "        target_cls = target_map[:, :-5, :, :]\n",
    "        target_obj = target_map[:, -5, :, :]\n",
    "        target_boxes = target_map[:, -4:, :, :]\n",
    "\n",
    "        # Compute losses\n",
    "        class_loss = F.binary_cross_entropy_with_logits(pred_cls, target_cls)\n",
    "        objectness_loss = F.binary_cross_entropy_with_logits(pred_obj, target_obj)\n",
    "        box_loss = F.l1_loss(pred_boxes, target_boxes)\n",
    "\n",
    "        return class_loss + objectness_loss + box_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "44182ecf-8401-4971-8268-7eb6f9e9a61c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Apply pruning to YOLOv11 model\n",
    "def apply_pruning(model, amount=0.5):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
    "            module.weight_mask.requires_grad = False  # Ensure mask is not updated\n",
    "\n",
    "# Function to enforce sparsity during training\n",
    "def enforce_sparsity(model):\n",
    "    with torch.no_grad():\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
    "                if hasattr(module, \"weight_mask\"):  # Check if pruning was applied\n",
    "                    module.weight *= module.weight_mask  # Reapply sparsity\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# def train_sparse_yolo_l1(model, dataloader, optimizer, device, num_epochs=5, l1_lambda=0.0001):\n",
    "    \n",
    "#     model = model.model.train()\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for batch in dataloader:\n",
    "#             # Move entire batch to device\n",
    "#             batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#             images = batch[\"img\"]  # Extract images from batch\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # Forward pass\n",
    "#             preds = model(images)\n",
    "            \n",
    "#             print(preds)\n",
    "            \n",
    "\n",
    "#             # Compute loss (including L1 regularization)\n",
    "#             loss, _ = model.loss(tuple(preds), batch)\n",
    "\n",
    "#             # L1 Regularization on model weights\n",
    "#             l1_loss = sum(param.abs().sum() for param in model.parameters())\n",
    "#             total_loss = loss + l1_lambda * l1_loss\n",
    "\n",
    "#             # Backpropagation\n",
    "#             total_loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "\n",
    "            \n",
    "#            print(f\"feats:{feats}\")\n",
    "            \n",
    "            # for xi in feats:\n",
    "            #     print(xi.shape)\n",
    "            \n",
    "            # print(f\"Preds type: {type(preds)}\")  # Check type of preds\n",
    "            # if isinstance(preds, list) or isinstance(preds, tuple):\n",
    "            #     print(f\"Preds length: {len(preds)}\")\n",
    "            #     print(f\"First element type: {type(preds[0])}\")\n",
    "            #     if isinstance(preds[0], torch.Tensor):\n",
    "            #         print(f\"Preds first shape: {preds[0].shape}\")\n",
    "            #     else:\n",
    "            #         print(f\"Preds first value: {preds[0]}\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "def train_sparse_yolo_l1(model, dataloader, optimizer, device, num_epochs=5, l1_lambda=0.0001):\n",
    "    \n",
    "    model= model.model.train()\n",
    "    model.dfl = False\n",
    "    print(f\"model_dfl:{model.dfl}\")\n",
    "    \n",
    "    print(\"Model num_classes:\", model.nc)\n",
    "\n",
    "    \n",
    "    custom_loss_fn = CustomYOLOLoss(num_classes=3).to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in dataloader:\n",
    "            images = batch[\"img\"].to(device)\n",
    "            targets = torch.cat([batch[\"batch_idx\"], batch[\"cls\"], batch[\"bboxes\"]], dim=1).to(device)\n",
    "\n",
    "            print(targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            preds = model(images)\n",
    "            print(preds[0].shape)\n",
    "            \n",
    "            # # Ensure preds is a tensor, not a list\n",
    "            # if isinstance(preds, list):\n",
    "            #     preds = preds[0]\n",
    "\n",
    "            # Compute loss using custom function\n",
    "            loss = custom_loss_fn(preds, targets)\n",
    "\n",
    "            # L1 Regularization on model weights\n",
    "            l1_loss = sum(param.abs().sum() for param in model.parameters())\n",
    "            total_loss = loss + l1_lambda * l1_loss\n",
    "\n",
    "            # Backpropagation\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db0ad218-d941-4dd9-8f1e-324e0f11174a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10b5591e-326d-4765-a385-7c7e1d5baf80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "lst=list(range(3))\n",
    "print(tuple(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1109b509-2b18-42e7-8da6-9eec2b2563be",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model=YOLO(\"models/yolo11x_trained_pruned_local_structured_30_channel.pt\").to(device)\n",
    "optimizer = torch.optim.AdamW(yolo_model.parameters(), lr=1e-4, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d4f8d845-bb27-49d6-b88a-25b86281610d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dfl:True\n",
      "Model num_classes: 3\n",
      "tensor([[0.0000, 1.0000, 0.5691, 0.5179, 0.3337, 0.4950]], device='cuda:0')\n",
      "torch.Size([1, 67, 80, 80])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([1, 3, 80, 80])) must be the same as input size (torch.Size([1, 62, 80, 80]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_sparse_yolo_l1\u001b[49m\u001b[43m(\u001b[49m\u001b[43myolo_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[96], line 102\u001b[0m, in \u001b[0;36mtrain_sparse_yolo_l1\u001b[0;34m(model, dataloader, optimizer, device, num_epochs, l1_lambda)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(preds[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# # Ensure preds is a tensor, not a list\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# if isinstance(preds, list):\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m#     preds = preds[0]\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Compute loss using custom function\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# L1 Regularization on model weights\u001b[39;00m\n\u001b[1;32m    105\u001b[0m l1_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(param\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[83], line 34\u001b[0m, in \u001b[0;36mCustomYOLOLoss.forward\u001b[0;34m(self, preds, targets)\u001b[0m\n\u001b[1;32m     31\u001b[0m target_boxes \u001b[38;5;241m=\u001b[39m target_map[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m:, :, :]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Compute losses\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m class_loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_cls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m objectness_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(pred_obj, target_obj)\n\u001b[1;32m     36\u001b[0m box_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39ml1_loss(pred_boxes, target_boxes)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:3160\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3157\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m   3162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([1, 3, 80, 80])) must be the same as input size (torch.Size([1, 62, 80, 80]))"
     ]
    }
   ],
   "source": [
    "train_sparse_yolo_l1(yolo_model,train_loader,optimizer=optimizer,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84e6b90e-0428-4e20-ae6b-251dd777be84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=yolo_model.model.train()\n",
    "\n",
    "for batch in train_loader:\n",
    "    images = batch[\"img\"]\n",
    "    cls=batch[\"cls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9867cd56-37cb-4f0f-8f2d-fe8972750f11",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(preds)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/nn/tasks.py:112\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/nn/tasks.py:130\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/nn/tasks.py:151\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 151\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    152\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/nn/modules/conv.py:50\u001b[0m, in \u001b[0;36mConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "preds=model(images)\n",
    "\n",
    "print(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
