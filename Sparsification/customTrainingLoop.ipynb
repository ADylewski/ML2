{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c9a7003-9498-462f-b979-ced5398165f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "class CocoaDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_files = [os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Load image as PIL.Image\n",
    "        image_path = self.image_files[index]\n",
    "        image = Image.open(image_path).convert('RGB')  # Ensure 3 channels (RGB)\n",
    "\n",
    "        # Attempt to load label\n",
    "        label_path = os.path.join(self.label_dir, os.path.basename(image_path).replace('.jpg', '.txt'))\n",
    "        if os.path.exists(label_path):\n",
    "            boxes = []\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    class_label, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                    boxes.append([class_label, x_center, y_center, width, height])\n",
    "            boxes = torch.tensor(boxes)\n",
    "        else:\n",
    "            boxes = torch.empty((0, 5))\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, boxes\n",
    "\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     images = []\n",
    "#     targets = []\n",
    "\n",
    "#     for i, (image, boxes) in enumerate(batch):\n",
    "#         images.append(image)\n",
    "\n",
    "#         if boxes.numel() > 0:  # If there are any boxes\n",
    "#             # Add batch index to boxes\n",
    "#             batch_indices = torch.full((boxes.size(0), 1), i, dtype=boxes.dtype)\n",
    "#             boxes = torch.cat((batch_indices, boxes), dim=1)\n",
    "        \n",
    "#         targets.append(boxes)\n",
    "\n",
    "#     # Stack images (images are already resized to the same size in the dataset)\n",
    "#     images = torch.stack(images, dim=0)\n",
    "\n",
    "#     # Concatenate all targets into a single tensor\n",
    "#     targets = torch.cat(targets, dim=0) if len(targets) > 0 else torch.empty((0, 6))\n",
    "\n",
    "#     return images, targets\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "\n",
    "    # Stack images\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Adjust targets\n",
    "    batched_targets = []\n",
    "    for i, target in enumerate(targets):\n",
    "        if len(target) > 0:\n",
    "            target[:, 0] = i  # Assign batch index\n",
    "            batched_targets.append(target)\n",
    "    batched_targets = torch.cat(batched_targets, dim=0) if batched_targets else torch.empty((0, 6))\n",
    "\n",
    "    return images, batched_targets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.model.train() # Set the model to training mode?\n",
    "    last_loss = 0\n",
    "    \n",
    "    \n",
    "    for images, targets in dataloader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(torch.float32).to(device)  # Ensure targets are float32 and on the correct device\n",
    "\n",
    "        # Skip batch if no targets\n",
    "        if targets.numel() == 0:\n",
    "            print(\"Skipping batch with no targets.\")\n",
    "            continue\n",
    "\n",
    "        # Debugging shapes\n",
    "        print(f\"Images shape: {images.shape}\")  # Should be [batch_size, 3, 640, 640]\n",
    "        print(f\"Targets shape: {targets.shape}\")  # Should be [num_annotations, 6]\n",
    "\n",
    "        # Forward pass\n",
    "        try:\n",
    "            loss, output = model(images, targets)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error during forward pass: {e}\")\n",
    "            print(f\"Images: {images.shape}\")\n",
    "            print(f\"Targets: {targets}\")\n",
    "            raise e\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        last_loss = loss.item()\n",
    "    \n",
    "    return last_loss\n",
    "    \n",
    "def validate(model, dataloader, device):\n",
    "    model.model.eval() # Set the model to evaluation mode?\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            loss, output = model(images, targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        average_loss = total_loss / len(dataloader)\n",
    "\n",
    "    return average_loss   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a28e3692-047c-4133-b953-2497691a1865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([64, 3, 640, 640])\n",
      "Targets shape: torch.Size([332, 5])\n",
      "Targets: tensor([[0.0000e+00, 7.0865e-01, 6.3233e-01, 7.3718e-02, 1.0505e-01],\n",
      "        [0.0000e+00, 8.7981e-01, 5.2873e-01, 6.0897e-02, 8.6298e-02],\n",
      "        [0.0000e+00, 5.6538e-01, 6.6983e-01, 3.2692e-02, 1.8510e-02],\n",
      "        ...,\n",
      "        [6.2000e+01, 9.6426e-01, 3.8798e-01, 4.9679e-02, 6.0096e-02],\n",
      "        [6.3000e+01, 6.2228e-01, 5.8798e-01, 1.7212e-01, 3.6202e-01],\n",
      "        [6.3000e+01, 2.9183e-01, 1.2368e-01, 2.1699e-01, 2.3053e-01]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (64) to match target batch_size (332).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m         loss\u001b[38;5;241m=\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 45\u001b[0m         \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#        loss, output = model(images, targets)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (64) to match target batch_size (332)."
     ]
    }
   ],
   "source": [
    "# Define transforms\n",
    "transforms = T.Compose([\n",
    "    T.Resize((640, 640)),  # Resize all images to 640x640\n",
    "    T.ToTensor()           # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "\n",
    "# Load dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True, collate_fn=collate_fn)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size = 64, shuffle = False, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize model, device, optimizer and other parameters\n",
    "model = YOLO('../yolo11n.pt').to('cuda')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "# Load training dataset\n",
    "dataset_dir = '../datasets/cocoa_diseases'\n",
    "train_dataset = CocoaDataset(\n",
    "    image_dir = os.path.join(dataset_dir, 'images/train'),\n",
    "    label_dir = os.path.join(dataset_dir, 'labels/train'),\n",
    "    transforms = transforms\n",
    ")\n",
    "\n",
    "# Load validation dataset\n",
    "val_dataset = CocoaDataset(\n",
    "    image_dir = os.path.join(dataset_dir, 'images/val'),\n",
    "    label_dir = os.path.join(dataset_dir, 'labels/val'),\n",
    "    transforms = transforms\n",
    ")\n",
    "\n",
    "\n",
    "for images, targets in train_loader:\n",
    "    images = images.to(device)\n",
    "    targets = targets.to(torch.float32).to(device)\n",
    "\n",
    "    print(f\"Images shape: {images.shape}\")\n",
    "    print(f\"Targets shape: {targets.shape}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "\n",
    "    try:\n",
    "        loss=CrossEntropyLoss()\n",
    "        loss(images, targets)\n",
    "#        loss, output = model(images, targets)\n",
    "        print(f\"Loss: {loss}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error during forward pass: {e}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b848073f-77f4-4f8d-a9d3-7cc174996794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([64, 3, 640, 640])\n",
      "Targets shape: torch.Size([335, 5])\n",
      "Error during forward pass: Boolean value of Tensor with more than one value is ambiguous\n",
      "Images: torch.Size([64, 3, 640, 640])\n",
      "Targets: tensor([[0.0000e+00, 4.4599e-01, 6.1875e-01, 1.8365e-01, 3.4760e-01],\n",
      "        [0.0000e+00, 5.2035e-01, 5.3125e-01, 1.4583e-01, 2.1635e-01],\n",
      "        [0.0000e+00, 6.1282e-01, 3.8678e-01, 1.7051e-01, 2.8942e-01],\n",
      "        ...,\n",
      "        [6.3000e+01, 7.3910e-01, 3.4916e-01, 7.0513e-03, 1.3702e-02],\n",
      "        [6.3000e+01, 7.3413e-01, 2.9663e-01, 1.1859e-02, 1.7308e-02],\n",
      "        [6.3000e+01, 1.2420e-01, 4.3690e-01, 1.7628e-02, 2.3798e-02]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training and validation loop\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 3\u001b[0m     last_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     average_val_loss \u001b[38;5;241m=\u001b[39m validate(model, val_loader, device)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Last Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_val_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 115\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImages: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTargets: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtargets\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    117\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    118\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[4], line 110\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     loss, output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during forward pass: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/engine/model.py:180\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    153\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    154\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/engine/model.py:558\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/engine/predictor.py:170\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs inference on an image or stream.\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m stream\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(num_epochs):\n",
    "    last_train_loss = train(model, train_loader, optimizer, device)\n",
    "    average_val_loss = validate(model, val_loader, device)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Last Training Loss: {last_train_loss:.4f}, Validation Loss: {average_val_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
